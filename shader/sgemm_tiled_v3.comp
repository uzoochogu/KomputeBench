// Derived from https://github.com/tpoisonooo/how-to-optimize-gemm/
// Tiled MatMul kernel (MMult_vk_2)

#version 450
#extension GL_EXT_control_flow_attributes : enable

#define TS 16u

layout(local_size_x = TS, local_size_y = TS) in;

layout(set = 0, binding = 0) readonly buffer buf_in_tensor_1 {
  float in_tensor_1[];
};
layout(set = 0, binding = 1) readonly buffer buf_in_tensor_2 {
  float in_tensor_2[];
};
layout(set = 0, binding = 2) writeonly buffer buf_out_tensor {
  float out_tensor[];
};

layout(constant_id = 0) const float tensor_size_f = 0;

shared float sub_tensor_1[TS][TS];
shared float sub_tensor_2[TS][TS];

void main() {
  uint block = TS;
  uint tensor_size = uint(tensor_size_f);
  uint loop = tensor_size / block;

  uvec3 threadID = gl_LocalInvocationID;

  uint globalCol = gl_WorkGroupID.y * block + threadID.y;
  uint globalRow = gl_WorkGroupID.x * block + threadID.x;

  float acc = 0.0;
  [[unroll]] for (uint i = 0u; i < loop; ++i) {
    sub_tensor_1[threadID.y][threadID.x] =
        in_tensor_1[tensor_size * globalCol + i * block + threadID.x];
    sub_tensor_2[threadID.y][threadID.x] =
        in_tensor_2[tensor_size * (i * block + threadID.y) + globalRow];

    // memoryBarrierShared(); // might be needed in some devices
    barrier();

#if 1
    for (uint k = 0u; k < block; ++k) {
      acc += sub_tensor_1[threadID.y][k] * sub_tensor_2[k][threadID.x];
    }
#else
    for (uint k = 0u; k < block; k += 4) {
      vec4 a;
      a.r = sub_tensor_1[threadID.y][k];
      a.g = sub_tensor_1[threadID.y][k + 1];
      a.b = sub_tensor_1[threadID.y][k + 2];
      a.a = sub_tensor_1[threadID.y][k + 3];

      vec4 b;
      b.r = sub_tensor_2[k][threadID.x];
      b.g = sub_tensor_2[k + 1][threadID.x];
      b.b = sub_tensor_2[k + 2][threadID.x];
      b.a = sub_tensor_2[k + 3][threadID.x];

      acc += dot(a, b);
    }
#endif
    barrier();
  }

  out_tensor[(globalCol * tensor_size) + globalRow] = acc;
}
